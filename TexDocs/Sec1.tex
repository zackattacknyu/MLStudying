%%
%% Class homework & solution template for latex
%% Alex Ihler
%%
\documentclass[twoside,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim,url}
\usepackage{listings}
\usepackage{bbm}
\usepackage{subcaption} 
\usepackage{upquote}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[scaled]{beramono}
%\usepackage{textcomp}

% Directories for other source files and images
\newcommand{\bibtexdir}{../bib}
\newcommand{\figdir}{fig}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\matlab}{{\sc Matlab}\ }

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.25in}  % Centers text.
\setlength{\evensidemargin}{-.25in} %
\setlength{\topmargin}{0in} %
\setlength{\headheight}{0in} %
\setlength{\headsep}{0in} %

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

\theoremstyle{definition}
\newtheorem{MatEx}{M{\scriptsize{ATLAB}} Usage Example}

\definecolor{comments}{rgb}{0,.5,0}
\definecolor{backgnd}{rgb}{.95,.95,.95}
\definecolor{string}{rgb}{.2,.2,.2}
\lstset{language=Matlab}
\lstset{basicstyle=\small\ttfamily,
        mathescape=true,
        emptylines=1, showlines=true,
        backgroundcolor=\color{backgnd},
        commentstyle=\color{comments}\ttfamily, %\rmfamily,
        stringstyle=\color{string}\ttfamily,
        keywordstyle=\ttfamily, %\normalfont,
        showstringspaces=false}
\newcommand{\matp}{\mathbf{\gg}}




\begin{document}

\section{Nearest Neighbor Methods}

\subsection{Nearest Neighbor for Classification}

We are given a set of n ordered pairs $(x_1,y_1)$, $(x_2,y_2)$, ..., $(x_n,y_n)$ where $y_i \in C$ with $C=\{ c_1, ..., c_M \}$ being a set of labels. The nearest neighbor classification $\hat{y}$ for a new point $x$ is $y_j$ where $j$ is the following:
\[
\arg \min_j||x-x_j||
\]
Let $x_1,x_2,...,x_k$ be the k closest points to $x$. Let $I$ be those indicies $1$ to $k$. The k-nearest nearest neighbor classification for $x$ is the label $c_m$ where $m$ is the following:
\[
\arg \max_m \mathbbm{1}_{I}{(y_i=c_m)}
\]

\subsection{Nearest Neighbor for Regression}

We are given a set of n ordered pairs $(x_1,y_1)$, $(x_2,y_2)$, ..., $(x_n,y_n)$ where $y_i \in \mathbbm{R}$. The nearest neighbor regression value $\hat{y}$ for a new point $x$ is $y_j$ where $j$ is the following:
\[
\arg \min_j||x-x_j||
\]
Let $x_1,x_2,...,x_k$ be the k closest points to $x$. Let $I$ be those indicies $1$ to $k$. The k-nearest nearest neighbor regression for $x$ is the mean of the $y_i$ values:
\[
\frac{1}{k} \sum_{i \in I} y_i
\]

\section{Bayes Classifier}

Estimate probability of class $p(y=c)$ using a probability model. 

\subsection{Multivariate Gaussian}

\subsection{Naive Bayes}
\newpage
\section{Linear Regression}

Define $\hat{y} = \theta x^T$\\
Let $X$ be set of $m$ points and $Y$ be their target, then error is as follows
\[
J(\theta) = \frac{1}{m} (Y - \theta X^T) \cdot (Y- \theta X^T)^T
\]
Or defined another way
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^m (y_i - \theta x_i^T)^2
\]
The partial with respect to paramater $\theta_k$ is as follows
\[
\frac{\partial J}{\partial \theta_k} = -\frac{2}{m} \sum_{i=1}^m (y_i - \theta x_i^T)x_{ik}
\]
The gradient vector for $J$ is thus
\[
\triangledown J(\theta) = -\frac{2}{m} (Y-\theta X^T) \cdot X
\]
To minimize the function, we set $\triangledown J(\theta) = 0$ and distribute so our equation becomes
\[
Y \cdot X = \theta X^T X
\]
Thus we finally have
\[
\hat{\theta} = Y \cdot X \cdot (X^T X)^{-1}
\]
The term $(X^T X)^{-1}$ is called the pseudo-inverse

\newpage

\section{Support Vector Machine}

Want to maximize the margin between two classes. \\
\\
Define function $f(x) = wx^T + b$ such that \\
$f(x)<-1$ for negative class and \\
$f(x)>1$ for positive class\\
\\
Define vector $w$ as perpendicular to the boundaries and closest two points on margin satisfy: \\
$w x_{pos} + b= +1$ and \\
$w x_{neg} + b = -1$\\
\\
For some scaler $r$ it will hold that $x_{pos} = x_{neg} + rw$\\
\\
We want to compute the size of the margin $M=||x_{pos}-x_{neg}|| = ||rw|| = r||w||$\\
\\
Need an expression for $r$ first.\\
Plugging in the expression for $-1$ into the expression for $+1$ we obtain
\[
w(x_{neg} + rw) + b = 1
\] 
Distributing this out we obtain
\[
w x_{neg} + r ||w||^2 + b = 1
\]
Collecting the terms for $-1$ we obtain
\[
r ||w||^2 - 1 = 1
\]
Thus we obtain
\[
r = \frac{2}{||w||^2}
\]
The size of the margin is now
\[
M = \frac{2}{\sqrt{w^Tw}}
\]
This means that we desire to minimize
\[
\sum_{j} w_j^2
\]
We need $w x^{(i)} + b \geq 1$ when $y^{(i)}=1$ and \\
$w x^{(i)} + b \leq -1$ when $y^{(i)}=-1$. We can\\
simplify this to $y^{(i)} (w x^{(i)} + b) \geq 1$

\newpage

\subsection{Primal Problem}

What we found previously means that finding the maximum margin classifier is equivalent to solving the following quadratic programming primal problem
\[
w^{*} = \arg \min_{w} \sum_j w_j^2
\]
with the following constraints:
\[
y^{(i)} ( w \cdot x^{(i)} + b) \geq 1
\]

\subsection{Primal Problem, not linearly separable}

We will have slack variable $\xi$ for violated constraints. The primal problem is now the following
\[
w^{*} = \arg \min_{w,\xi} \sum_j w_j^2 + R \sum_i \xi^{(i)}
\]
subject to the following constraints
\[
y^{(i)} (w x^{(i)} + b) \geq 1 - \xi^{(i)}
\]
\[
\xi^{(i)} \geq 0
\]



\end{document}
