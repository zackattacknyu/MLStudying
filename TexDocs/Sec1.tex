%%
%% Class homework & solution template for latex
%% Alex Ihler
%%
\documentclass[twoside,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim,url}
\usepackage{listings}
\usepackage{bbm}
\usepackage{subcaption} 
\usepackage{upquote}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[scaled]{beramono}
%\usepackage{textcomp}

% Directories for other source files and images
\newcommand{\bibtexdir}{../bib}
\newcommand{\figdir}{fig}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\matlab}{{\sc Matlab}\ }

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.25in}  % Centers text.
\setlength{\evensidemargin}{-.25in} %
\setlength{\topmargin}{0in} %
\setlength{\headheight}{0in} %
\setlength{\headsep}{0in} %

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

\theoremstyle{definition}
\newtheorem{MatEx}{M{\scriptsize{ATLAB}} Usage Example}

\definecolor{comments}{rgb}{0,.5,0}
\definecolor{backgnd}{rgb}{.95,.95,.95}
\definecolor{string}{rgb}{.2,.2,.2}
\lstset{language=Matlab}
\lstset{basicstyle=\small\ttfamily,
        mathescape=true,
        emptylines=1, showlines=true,
        backgroundcolor=\color{backgnd},
        commentstyle=\color{comments}\ttfamily, %\rmfamily,
        stringstyle=\color{string}\ttfamily,
        keywordstyle=\ttfamily, %\normalfont,
        showstringspaces=false}
\newcommand{\matp}{\mathbf{\gg}}




\begin{document}

\section{Nearest Neighbor Methods}

\subsection{Nearest Neighbor for Classification}

We are given a set of n ordered pairs $(x_1,y_1)$, $(x_2,y_2)$, ..., $(x_n,y_n)$ where $y_i \in C$ with $C=\{ c_1, ..., c_M \}$ being a set of labels. The nearest neighbor classification $\hat{y}$ for a new point $x$ is $y_j$ where $j$ is the following:
\[
\arg \min_j||x-x_j||
\]
Let $x_1,x_2,...,x_k$ be the k closest points to $x$. Let $I$ be those indicies $1$ to $k$. The k-nearest nearest neighbor classification for $x$ is the label $c_m$ where $m$ is the following:
\[
\arg \max_m \mathbbm{1}_{I}{(y_i=c_m)}
\]

\subsection{Nearest Neighbor for Regression}

We are given a set of n ordered pairs $(x_1,y_1)$, $(x_2,y_2)$, ..., $(x_n,y_n)$ where $y_i \in \mathbbm{R}$. The nearest neighbor regression value $\hat{y}$ for a new point $x$ is $y_j$ where $j$ is the following:
\[
\arg \min_j||x-x_j||
\]
Let $x_1,x_2,...,x_k$ be the k closest points to $x$. Let $I$ be those indicies $1$ to $k$. The k-nearest nearest neighbor regression for $x$ is the mean of the $y_i$ values:
\[
\frac{1}{k} \sum_{i \in I} y_i
\]

\section{Bayes Classifier}

Estimate probability of class $p(y=c)$ using a probability model. 

\subsection{Multivariate Gaussian}

\subsection{Naive Bayes}
\newpage
\section{Linear Regression}

Define $\hat{y} = \theta x^T$\\
Let $X$ be set of $m$ points and $Y$ be their target, then error is as follows
\[
J(\theta) = \frac{1}{m} (Y - \theta X^T) \cdot (Y- \theta X^T)^T
\]
Or defined another way
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^m (y_i - \theta x_i^T)^2
\]
The partial with respect to paramater $\theta_k$ is as follows
\[
\frac{\partial J}{\partial \theta_k} = -\frac{2}{m} \sum_{i=1}^m (y_i - \theta x_i^T)x_{ik}
\]
The gradient vector for $J$ is thus
\[
\triangledown J(\theta) = -\frac{2}{m} (Y-\theta X^T) \cdot X
\]
To minimize the function, we set $\triangledown J(\theta) = 0$ and distribute so our equation becomes
\[
Y \cdot X = \theta X^T X
\]
Thus we finally have
\[
\hat{\theta} = Y \cdot X \cdot (X^T X)^{-1}
\]
The term $(X^T X)^{-1}$ is called the pseudo-inverse


\end{document}
