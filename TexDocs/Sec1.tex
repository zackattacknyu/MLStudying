%%
%% Class homework & solution template for latex
%% Alex Ihler
%%
\documentclass[twoside,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim,url}
\usepackage{listings}
\usepackage{bbm}
\usepackage{subcaption} 
\usepackage{upquote}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[scaled]{beramono}
%\usepackage{textcomp}

% Directories for other source files and images
\newcommand{\bibtexdir}{../bib}
\newcommand{\figdir}{fig}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\matlab}{{\sc Matlab}\ }

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.25in}  % Centers text.
\setlength{\evensidemargin}{-.25in} %
\setlength{\topmargin}{0in} %
\setlength{\headheight}{0in} %
\setlength{\headsep}{0in} %

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

\theoremstyle{definition}
\newtheorem{MatEx}{M{\scriptsize{ATLAB}} Usage Example}

\definecolor{comments}{rgb}{0,.5,0}
\definecolor{backgnd}{rgb}{.95,.95,.95}
\definecolor{string}{rgb}{.2,.2,.2}
\lstset{language=Matlab}
\lstset{basicstyle=\small\ttfamily,
        mathescape=true,
        emptylines=1, showlines=true,
        backgroundcolor=\color{backgnd},
        commentstyle=\color{comments}\ttfamily, %\rmfamily,
        stringstyle=\color{string}\ttfamily,
        keywordstyle=\ttfamily, %\normalfont,
        showstringspaces=false}
\newcommand{\matp}{\mathbf{\gg}}




\begin{document}

\section{Nearest Neighbor Methods}

\subsection{Nearest Neighbor for Classification}

We are given a set of n ordered pairs $(x_1,y_1)$, $(x_2,y_2)$, ..., $(x_n,y_n)$ where $y_i \in C$ with $C=\{ c_1, ..., c_M \}$ being a set of labels. The nearest neighbor classification $\hat{y}$ for a new point $x$ is $y_j$ where $j$ is the following:
\[
\arg \min_j||x-x_j||
\]
Let $x_1,x_2,...,x_k$ be the k closest points to $x$. Let $I$ be those indicies $1$ to $k$. The k-nearest nearest neighbor classification for $x$ is the label $c_m$ where $m$ is the following:
\[
\arg \max_m \mathbbm{1}_{I}{(y_i=c_m)}
\]

\subsection{Nearest Neighbor for Regression}

We are given a set of n ordered pairs $(x_1,y_1)$, $(x_2,y_2)$, ..., $(x_n,y_n)$ where $y_i \in \mathbbm{R}$. The nearest neighbor regression value $\hat{y}$ for a new point $x$ is $y_j$ where $j$ is the following:
\[
\arg \min_j||x-x_j||
\]
Let $x_1,x_2,...,x_k$ be the k closest points to $x$. Let $I$ be those indicies $1$ to $k$. The k-nearest nearest neighbor regression for $x$ is the mean of the $y_i$ values:
\[
\frac{1}{k} \sum_{i \in I} y_i
\]

\newpage

\section{Bayes Classifier}

Estimate probability of class $p(y=c)$ using a probability model. 

\subsection{Gaussian Classifier}

Take $m$ training points $x^{(i)}$ that have class $c$, then the parameters of the Gaussian for $c$ is as follows
\[
\mu_c = \frac{1}{m} \sum_{i} x^{(i)}
\]
\[
\sigma_c^2 = \frac{1}{m} \sum_{i} (x^{(i)} - \mu_c)^2
\]

\subsection{Multivariate Gaussian}

Let $x$ and $\mu$ be vectors, then the multivariate Gaussian is defined as follows
\[
N(x; \mu, \Sigma) = \frac{1}{\sqrt{(2 \pi)^d |\Sigma|} } \exp[ -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) ]
\]
The MLE estimates for the parameters are as follows: 
\[
\hat{\mu} = \frac{1}{m} \sum_i x^{(i)}
\]
\[
\hat{\Sigma} = \frac{1}{m} \sum_i (x^{(i)}-\hat{\mu})^T(x^{(i)}-\hat{\mu})
\]

\subsection{Naive Bayes}

We assume each feature $x$ is independent given class of $y$, thus 
\[
p(y|x) = \frac{p(y)\prod_i p(x_i|y)}{p(x)}
\]

\subsection{Bayes Error Rate}

Optimal decision at particular $x$ is as follows
\[
\hat{y} = \arg \max_c p(y=c|x)
\]
Bayes Error Rate is as follows
\[
E_{xy} [y \neq \hat{y} ] = E_x [1 - \max_c p(y=c | x) ]
\]
This is the best any classifier can do and it measures hardness of separating y-values
\newpage

\section{Linear Classifier}

\subsection{Perceptron}
Simple 2-class classifier with a decision boundary defined as
\[
\theta x^T = 0
\]
Data is separable if there is a perceptron that correctly predicts all points

\subsection{Perceptron Algorithm}
Following learning algorithm is used since linear regression can distort solution:\\
\begin{lstlisting}
while not done:
	for all data points i:
		$\hat{y}^{(i)} = T(\theta x^{(i)})$
		$\theta \leftarrow \theta + \alpha(y^{(i)}-\hat{y}^{(i)})x^{(i)}$
\end{lstlisting}


\newpage
\section{Linear Regression}

Define $\hat{y} = \theta x^T$\\
Let $X$ be set of $m$ points and $Y$ be their target, then error is as follows
\[
J(\theta) = \frac{1}{m} (Y - \theta X^T) \cdot (Y- \theta X^T)^T
\]
Or defined another way
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^m (y_i - \theta x_i^T)^2
\]
The partial with respect to paramater $\theta_k$ is as follows
\[
\frac{\partial J}{\partial \theta_k} = -\frac{2}{m} \sum_{i=1}^m (y_i - \theta x_i^T)x_{ik}
\]
The gradient vector for $J$ is thus
\[
\triangledown J(\theta) = -\frac{2}{m} (Y-\theta X^T) \cdot X
\]
To minimize the function, we set $\triangledown J(\theta) = 0$ and distribute so our equation becomes
\[
Y \cdot X = \theta X^T X
\]
Thus we finally have
\[
\hat{\theta} = Y \cdot X \cdot (X^T X)^{-1}
\]
The term $(X^T X)^{-1}$ is called the pseudo-inverse

\newpage

\section{Support Vector Machine}

Want to maximize the margin between two classes. \\
\\
Define function $f(x) = wx^T + b$ such that \\
$f(x)<-1$ for negative class and \\
$f(x)>1$ for positive class\\
\\
Define vector $w$ as perpendicular to the boundaries and closest two points on margin satisfy: \\
$w x_{pos} + b= +1$ and \\
$w x_{neg} + b = -1$\\
\\
For some scaler $r$ it will hold that $x_{pos} = x_{neg} + rw$\\
\\
We want to compute the size of the margin $M=||x_{pos}-x_{neg}|| = ||rw|| = r||w||$\\
\\
Need an expression for $r$ first.\\
Plugging in the expression for $-1$ into the expression for $+1$ we obtain
\[
w(x_{neg} + rw) + b = 1
\] 
Distributing this out we obtain
\[
w x_{neg} + r ||w||^2 + b = 1
\]
Collecting the terms for $-1$ we obtain
\[
r ||w||^2 - 1 = 1
\]
Thus we obtain
\[
r = \frac{2}{||w||^2}
\]
The size of the margin is now
\[
M = \frac{2}{\sqrt{w^Tw}}
\]
This means that we desire to minimize
\[
\sum_{j} w_j^2
\]
We need $w x^{(i)} + b \geq 1$ when $y^{(i)}=1$ and \\
$w x^{(i)} + b \leq -1$ when $y^{(i)}=-1$. We can\\
simplify this to $y^{(i)} (w x^{(i)} + b) \geq 1$

\newpage

\subsection{Primal Problem}

What we found previously means that finding the maximum margin classifier is equivalent to solving the following quadratic programming primal problem
\[
w^{*} = \arg \min_{w} \sum_j w_j^2
\]
with the following constraints:
\[
y^{(i)} ( w \cdot x^{(i)} + b) \geq 1
\]

\subsection{Primal Problem, not linearly separable}

We will have slack variable $\xi$ for violated constraints. The primal problem is now the following
\[
w^{*} = \arg \min_{w,\xi} \sum_j w_j^2 + R \sum_i \xi^{(i)}
\]
subject to the following constraints
\[
y^{(i)} (w x^{(i)} + b) \geq 1 - \xi^{(i)}
\]
\[
\xi^{(i)} \geq 0
\]

\subsection{Lagrange Multipliers for Primal}

Let $f(\theta) = \sum_j {w_j^2}$\\
Let $g_i(\theta) = 1-y^{(i)}(w \cdot x^{(i)} + b)$\\
We will introduce the Lagrange multipliers $\alpha_i$\\
\[
\theta^* = \arg \min_{\theta} \max_{\alpha \geq 0} \frac{1}{2} f(\theta) + \sum_i \alpha_i g_i(\theta)
\]

Since we want points where $\nabla f = \alpha \nabla g$, it holds that 
\[
w^* = \sum_i \alpha_i y^{(i)} x^{(i)}
\]
Since it holds that $y=wx+b$, we have
\[
b = \frac{1}{N_{sv}} \sum_{i \in SV} (y^{(i)} - w \cdot x^{(i)})
\]

\subsection{Dual Form for SVM}

We use the $w^*$ solution above to write maximize in terms of only $\alpha$ 
\[
\max_{\alpha \geq 0} \sum_i [\alpha_i - \frac{1}{2} \sum_j \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)} \cdot x^{(j)})]
\]
such that
\[
\sum_i \alpha_i y^{(i)} = 0
\]
Combine that constraint with the following and we have a quadratic program with $m$ variables and $m+1$ constraints 
\[
w^* = \sum_i \alpha_i y^{(i)} x^{(i)}
\]
\[
b = \frac{1}{N_{sv}} \sum_{i \in SV} (y^{(i)} - w \cdot x^{(i)})
\]

\newpage

\section{Principal Component Analysis}

Take data points $x^{(i)}$\\
Want to find vectors $v_1,...,v_M$ that most closely reconstruct it\\
Let $a_1,...,a_M$ be set of scalers\\
This problem reduces to
\[
\min_{v,a} \sum_{i} (x^{(i)} - \sum_{j=1}^M a_j^{(i)}v_j)^2
\]

\subsection{Eigendecomposition}

This problem is equivalent to finding the ellipsoid that covers the both and computing its orthogonal basis. This means finding the covariance matrix. In the end, the following procedure is done:\\
\\
1. Subtract each data by its mean to obtain matrix $X$ of data\\
2. Scale each dimension by its variance\\
3. Find the covariance matrix S
\[
S = \frac{1}{n} (X-m)^T(X-m)
\]
4. Compute the k largest eigenvectors of S to get the k principal components

\subsection{Singular Value Decomposition}

Most common method of finding principal components\\
Decompose matrix into following $X=USV^T$\\
The coefficients are obtained from $U \cdot S$ and the $V$ matrix provides the principal vectors

\end{document}
